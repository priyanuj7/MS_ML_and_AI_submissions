{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91375,"databundleVersionId":10790442,"sourceType":"competition"}],"dockerImageVersionId":30886,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Importing the necessary Libraries","metadata":{}},{"cell_type":"code","source":"#Data Structures\nimport pandas as pd\nimport numpy as np\nimport re\nimport os\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\n### For installing missingno library, type this command in terminal\n#pip install missingno\n\nimport missingno as msno\nfrom scipy import stats\n\n#Sklearn\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score\nfrom sklearn.metrics import accuracy_score, classification_report\n\n#Plotting\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\n\n#Others\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:54:14.139276Z","iopub.execute_input":"2025-02-09T13:54:14.139654Z","iopub.status.idle":"2025-02-09T13:54:14.149829Z","shell.execute_reply.started":"2025-02-09T13:54:14.139623Z","shell.execute_reply":"2025-02-09T13:54:14.148531Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Loading the data","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:54:14.151524Z","iopub.execute_input":"2025-02-09T13:54:14.151947Z","iopub.status.idle":"2025-02-09T13:54:14.181181Z","shell.execute_reply.started":"2025-02-09T13:54:14.151901Z","shell.execute_reply":"2025-02-09T13:54:14.179831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/telecom-churn-case-study-hackathon-c-68/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/telecom-churn-case-study-hackathon-c-68/test.csv\")\nsample_output = pd.read_csv(\"/kaggle/input/telecom-churn-case-study-hackathon-c-68/sample.csv\")\ndata_dict = pd.read_csv(\"/kaggle/input/telecom-churn-case-study-hackathon-c-68/data_dictionary.csv\")\n\nprint('The shape of the train dataset is: ', train_df.shape)\nprint('The shape of the test dataset is: ', test_df.shape)\nprint('The shape of the sample output is: ', sample_output.shape)\nprint('The shape of the data dictionary is: ', data_dict.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:54:14.183135Z","iopub.execute_input":"2025-02-09T13:54:14.183584Z","iopub.status.idle":"2025-02-09T13:54:17.164101Z","shell.execute_reply.started":"2025-02-09T13:54:14.183540Z","shell.execute_reply":"2025-02-09T13:54:17.162783Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Loading the data dictionary for understanding the data at an uber level","metadata":{}},{"cell_type":"code","source":"data_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:54:17.165417Z","iopub.execute_input":"2025-02-09T13:54:17.165827Z","iopub.status.idle":"2025-02-09T13:54:17.195678Z","shell.execute_reply.started":"2025-02-09T13:54:17.165789Z","shell.execute_reply":"2025-02-09T13:54:17.194466Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Separating the dependent and independent variable","metadata":{}},{"cell_type":"markdown","source":"#### Doing a basic df.head() to do an overview of the dataframes","metadata":{}},{"cell_type":"code","source":"train_df.head(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:54:17.198325Z","iopub.execute_input":"2025-02-09T13:54:17.198655Z","iopub.status.idle":"2025-02-09T13:54:17.287809Z","shell.execute_reply.started":"2025-02-09T13:54:17.198621Z","shell.execute_reply":"2025-02-09T13:54:17.286736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.head(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:54:17.289320Z","iopub.execute_input":"2025-02-09T13:54:17.289710Z","iopub.status.idle":"2025-02-09T13:54:17.378972Z","shell.execute_reply.started":"2025-02-09T13:54:17.289683Z","shell.execute_reply":"2025-02-09T13:54:17.377886Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Since, we have to submit the model scores on test set, therefore, we need to create the validation set in order to understand how good the model is.","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(train_df.drop('churn_probability', axis = 1),train_df['churn_probability'] , \n                                   random_state=104,  \n                                   test_size=0.10,  \n                                   shuffle=True) \n\nid_train = X_train[['id']]\nid_val = X_val[['id']]\n\nX_train.drop('id', axis = 1, inplace = True)\nX_val.drop('id', axis = 1, inplace = True)\n\n\nprint('Shape of X_train is: ', X_train.shape)\nprint('Shape of y_train is: ', y_train.shape)\nprint('Shape of X_val is: ', X_val.shape)\nprint('Shape of y_val is: ', y_val.shape)\n\n#Changing the dataset name for maintaining the consistency of format throughout\nX_test = test_df.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:54:17.380302Z","iopub.execute_input":"2025-02-09T13:54:17.380680Z","iopub.status.idle":"2025-02-09T13:54:17.566733Z","shell.execute_reply.started":"2025-02-09T13:54:17.380641Z","shell.execute_reply":"2025-02-09T13:54:17.565658Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### EDA and basic feature selection performed on the train dataset only","metadata":{}},{"cell_type":"code","source":"class DataFrameAnalyzer:\n    def __init__(self, df):\n        \"\"\"Initialize with a DataFrame.\"\"\"\n        self.df = df\n    \n    def basics(self):\n        \"\"\"Prints the DataFrame description.\"\"\"\n        print(\"\\nDataFrame Description:\")\n        print(self.df.shape)\n    \n    def percentage_nulls(self):\n        \"\"\"Prints the percentage of nulls per column.\"\"\"\n        print(\"\\nPercentage of Null Values Per Column:\")\n        print((self.df.isnull().sum() * 100)/self.df.shape[0])\n    \n    def single_value_columns(self):\n        \"\"\"Prints columns that have the same single value throughout.\"\"\"\n        print(\"\\nColumns with a Single Unique Value:\")\n        single_value_cols = [col for col in self.df.columns if self.df[col].nunique() == 1]\n        print(single_value_cols if single_value_cols else \"None\")\n\n    def drop_high_null_columns(self, threshold):\n        \"\"\"Drops columns where the percentage of null values is higher than the given threshold.\"\"\"\n        high_null_cols = [col for col in self.df.columns if ((self.df[col].isnull().sum() * 100)/self.df.shape[0]) > threshold]\n        self.df.drop(columns=high_null_cols, inplace=True)\n        print(\"\\nDropped Columns with High Null Values:\", high_null_cols)\n    \n    def drop_single_value_columns(self):\n        \"\"\"Drops columns that contain only a single unique value throughout.\"\"\"\n        single_value_cols = [col for col in self.df.columns if self.df[col].nunique() == 1]\n        self.df.drop(columns=single_value_cols, inplace=True)\n        print(\"\\nDropped Columns with Single Unique Value:\", single_value_cols)\n\ninitial_analyzer = DataFrameAnalyzer(X_train)\ninitial_analyzer.basics()\ninitial_analyzer.percentage_nulls()\ninitial_analyzer.single_value_columns()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:54:17.567806Z","iopub.execute_input":"2025-02-09T13:54:17.568085Z","iopub.status.idle":"2025-02-09T13:54:17.838484Z","shell.execute_reply.started":"2025-02-09T13:54:17.568060Z","shell.execute_reply":"2025-02-09T13:54:17.837401Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Immediately on the basis of preliminary EDA, we can remove the columns with greater than 70% null values as they will not provide enough information to ML models, additionally, we will also be removing the columns which have single value throughout.\n\n#### However, need to check if there are some categorical columns, then we do not need to delete and can do fillna with 0","metadata":{}},{"cell_type":"code","source":"# List of columns for check if they are categorical and need not require deletion even if nulls > 70 %\ncat_check = ['night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', 'fb_user_6', 'fb_user_7', 'fb_user_8']\nfor col in cat_check:\n    print(col, train_df[col].value_counts())\nprint('--'*60)\n# If a lot of the values are unique and then we have 74% null values, we can delete them\nunique_check = ['date_of_last_rech_data_6', 'date_of_last_rech_data_7', 'date_of_last_rech_data_8', 'total_rech_data_6', 'total_rech_data_7', 'total_rech_data_8', 'max_rech_data_6', 'max_rech_data_7', 'max_rech_data_8', 'count_rech_2g_6', 'count_rech_2g_7', 'count_rech_2g_8', 'count_rech_3g_6', 'count_rech_3g_7', 'count_rech_3g_8', 'av_rech_amt_data_6', 'av_rech_amt_data_7', 'av_rech_amt_data_8', 'arpu_3g_6', 'arpu_3g_7', 'arpu_3g_8', 'arpu_2g_6', 'arpu_2g_7', 'arpu_2g_8']\nfor col in unique_check:\n    print(col, train_df[col].nunique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:54:17.839523Z","iopub.execute_input":"2025-02-09T13:54:17.839880Z","iopub.status.idle":"2025-02-09T13:54:17.896569Z","shell.execute_reply.started":"2025-02-09T13:54:17.839851Z","shell.execute_reply":"2025-02-09T13:54:17.895527Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# In order to not drop these columns in the below method call, already replacing the columns null values by 0.\n# Later these features can be leveraged for feature engineering\n\nX_train[['night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', 'fb_user_6', 'fb_user_7', 'fb_user_8']] = X_train[['night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', 'fb_user_6', 'fb_user_7', 'fb_user_8']].fillna(0, axis = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:54:17.899348Z","iopub.execute_input":"2025-02-09T13:54:17.899684Z","iopub.status.idle":"2025-02-09T13:54:17.921407Z","shell.execute_reply.started":"2025-02-09T13:54:17.899656Z","shell.execute_reply":"2025-02-09T13:54:17.919900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"initial_analyzer.drop_high_null_columns(70)\ninitial_analyzer.drop_single_value_columns()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:54:17.923519Z","iopub.execute_input":"2025-02-09T13:54:17.923945Z","iopub.status.idle":"2025-02-09T13:54:18.242368Z","shell.execute_reply.started":"2025-02-09T13:54:17.923917Z","shell.execute_reply":"2025-02-09T13:54:18.241115Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking if we have removed the required columns\nprint(X_train.shape)\nX_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:54:18.243812Z","iopub.execute_input":"2025-02-09T13:54:18.244131Z","iopub.status.idle":"2025-02-09T13:54:18.346295Z","shell.execute_reply.started":"2025-02-09T13:54:18.244104Z","shell.execute_reply":"2025-02-09T13:54:18.345212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CorrelationAnalyzer:\n    def __init__(self, df):\n        \"\"\"Initialize with a DataFrame.\"\"\"\n        self.df = df\n    \n    def get_correlation_dataframe(self):\n        \"\"\"Returns a DataFrame with correlations between numerical features.\"\"\"\n        corr_matrix = self.df.corr().stack().reset_index()\n        corr_matrix.columns = ['Variable1', 'Variable2', 'Correlation']\n        corr_matrix = corr_matrix[corr_matrix['Variable1'] != corr_matrix['Variable2']]\n        return corr_matrix\n\ncorr_df = CorrelationAnalyzer(X_train.drop(['date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8'], axis = 1))\ncorr_df = corr_df.get_correlation_dataframe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:54:18.347365Z","iopub.execute_input":"2025-02-09T13:54:18.347767Z","iopub.status.idle":"2025-02-09T13:54:21.329561Z","shell.execute_reply.started":"2025-02-09T13:54:18.347734Z","shell.execute_reply":"2025-02-09T13:54:21.328575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corr_df = corr_df.sort_values(by = 'Correlation', ascending = False)\ncorr_df = corr_df.iloc[1::2]\ncorr_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:54:21.330590Z","iopub.execute_input":"2025-02-09T13:54:21.330961Z","iopub.status.idle":"2025-02-09T13:54:21.893622Z","shell.execute_reply.started":"2025-02-09T13:54:21.330925Z","shell.execute_reply":"2025-02-09T13:54:21.892102Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### As there are a lot of columns, planning to drop correlated variables upto an extent of 0.80. We can tweak this as per result","metadata":{}},{"cell_type":"code","source":"variable1_80 = corr_df[corr_df['Correlation']>.8]['Variable1'].to_list()\nvariable2_80 = corr_df[corr_df['Correlation']>.8]['Variable2'].to_list()\n\n# Checking if there are any common that needs to be kept\nset(variable1_80).intersection(set(variable2_80))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:54:21.895001Z","iopub.execute_input":"2025-02-09T13:54:21.895291Z","iopub.status.idle":"2025-02-09T13:54:21.904718Z","shell.execute_reply.started":"2025-02-09T13:54:21.895268Z","shell.execute_reply":"2025-02-09T13:54:21.903556Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### We can now drop all variables from the second list as we will still retain information from the first list","metadata":{}},{"cell_type":"code","source":"X_train.drop(variable2_80, inplace = True, axis = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:54:21.905620Z","iopub.execute_input":"2025-02-09T13:54:21.905892Z","iopub.status.idle":"2025-02-09T13:54:21.942927Z","shell.execute_reply.started":"2025-02-09T13:54:21.905870Z","shell.execute_reply":"2025-02-09T13:54:21.941940Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X_train.shape)\nX_train.head(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:54:21.943948Z","iopub.execute_input":"2025-02-09T13:54:21.944294Z","iopub.status.idle":"2025-02-09T13:54:22.014005Z","shell.execute_reply.started":"2025-02-09T13:54:21.944259Z","shell.execute_reply":"2025-02-09T13:54:22.012884Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Indepth EDA and analysis to understand variables","metadata":{}},{"cell_type":"code","source":"def plot_numeric_distributions(df, target_column = 'churn_probability'):\n    \"\"\"\n    Plots the distribution of numerical columns across the categorical target column (0 and 1).\n    \n    Parameters:\n    df (pd.DataFrame): Input dataframe\n    target_column (str): The categorical column with 0 and 1 values\n    \"\"\"\n    numeric_cols = df.drop(['date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8'], axis = 1).columns\n\n    df = pd.concat([df, pd.DataFrame(y_train)], axis = 1)\n    \n    for col in numeric_cols:\n        plt.figure(figsize=(8, 5))\n        sns.histplot(data=df, x=col, hue=target_column, kde=True, bins=30, alpha=0.5)\n        plt.title(f\"Distribution of {col} by {target_column}\")\n        plt.legend(title=target_column, labels=[0, 1])\n        plt.show()\n\nplot_numeric_distributions(X_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:54:22.015070Z","iopub.execute_input":"2025-02-09T13:54:22.015365Z","iopub.status.idle":"2025-02-09T13:55:26.927163Z","shell.execute_reply.started":"2025-02-09T13:54:22.015341Z","shell.execute_reply":"2025-02-09T13:55:26.926132Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### At the first go with the graphs, we see that most of the remaining variables do show some type of pattern, therefore, rather than feature selection, we can also approach via PCA","metadata":{}},{"cell_type":"markdown","source":"### Outlier Treatment\n\nBefore going with PCA, as there are a lot of columns it is better to see which columns have how many outliers and further remove reduce or impute them. Hence doing outlier treatment well beforehand","metadata":{}},{"cell_type":"code","source":"def detect_outliers_boxplot(data):\n    outliers = {}\n    for col in data.columns:\n        Q1 = data[col].quantile(0.25)\n        Q3 = data[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        outliers[col] = data[(data[col] < lower_bound) | (data[col] > upper_bound)].shape[0]\n    return outliers\n\n# Function to detect outliers based on Z-score criteria\ndef detect_outliers_zscore(data, threshold=3):\n    outliers = {}\n    for col in data.columns:\n        z_scores = np.abs(stats.zscore(data[col]))\n        outliers[col] = len(np.where(z_scores > threshold)[0])\n    return outliers\n\n# Function for outlier treatment: You can either remove or cap the outliers\ndef treat_outliers(data, method='remove', criteria='boxplot', threshold=3):\n    if criteria == 'boxplot':\n        outliers = detect_outliers_boxplot(data)\n        for col in outliers:\n            Q1 = data[col].quantile(0.25)\n            Q3 = data[col].quantile(0.75)\n            IQR = Q3 - Q1\n            lower_bound = Q1 - 1.5 * IQR\n            upper_bound = Q3 + 1.5 * IQR\n            if method == 'remove':\n                data = data[(data[col] >= lower_bound) & (data[col] <= upper_bound)]\n            elif method == 'cap':\n                data[col] = np.where(data[col] > upper_bound, upper_bound, data[col])\n                data[col] = np.where(data[col] < lower_bound, lower_bound, data[col])\n\n    elif criteria == 'zscore':\n        for col in data.columns:\n            z_scores = np.abs(stats.zscore(data[col]))\n            if method == 'remove':\n                data = data[z_scores < threshold]\n            elif method == 'cap':\n                data[col] = np.where(z_scores > threshold, np.sign(data[col]) * threshold * data[col].std(), data[col])\n    \n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:55:26.928416Z","iopub.execute_input":"2025-02-09T13:55:26.928828Z","iopub.status.idle":"2025-02-09T13:55:26.941003Z","shell.execute_reply.started":"2025-02-09T13:55:26.928791Z","shell.execute_reply":"2025-02-09T13:55:26.939820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Detect outliers using both methods\nboxplot_outliers = detect_outliers_boxplot(X_train.drop(['date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8'], axis = 1))\nzscore_outliers = detect_outliers_zscore(X_train.drop(['date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8'], axis = 1))\n\n# Print outliers count by column\nprint(\"Outliers by Boxplot Criteria:\", boxplot_outliers)\nprint('--'*60)\nprint(\"Outliers by Z-score Criteria:\", zscore_outliers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:55:26.942204Z","iopub.execute_input":"2025-02-09T13:55:26.942633Z","iopub.status.idle":"2025-02-09T13:55:28.400107Z","shell.execute_reply.started":"2025-02-09T13:55:26.942592Z","shell.execute_reply":"2025-02-09T13:55:28.398976Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(np.mean(list(boxplot_outliers.values())))\nprint(np.mean(list(zscore_outliers.values())))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:55:28.401407Z","iopub.execute_input":"2025-02-09T13:55:28.401820Z","iopub.status.idle":"2025-02-09T13:55:28.408143Z","shell.execute_reply.started":"2025-02-09T13:55:28.401783Z","shell.execute_reply":"2025-02-09T13:55:28.407163Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### As of now, it seems we might not loose a lot of rows if we are removing the outliers via zscore method. We will follow this process and remove the rows. We will also check the final percentage of rows we lost.","metadata":{}},{"cell_type":"code","source":"outlier_trt_rqd = ['arpu_7', 'total_rech_num_6', 'total_rech_num_7', 'total_rech_num_8', 'total_rech_amt_6', 'total_rech_amt_8'\n                   , 'max_rech_amt_6', 'max_rech_amt_7', 'max_rech_amt_8', 'last_day_rch_amt_6', 'last_day_rch_amt_7'\n                   , 'last_day_rch_amt_8', 'vol_2g_mb_6', 'vol_2g_mb_7', 'vol_2g_mb_8', 'vol_3g_mb_6', 'vol_3g_mb_7'\n                   , 'vol_3g_mb_8', 'night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', 'monthly_2g_6', 'monthly_2g_7'\n                   , 'monthly_2g_8', 'sachet_2g_6', 'sachet_2g_7', 'sachet_2g_8', 'monthly_3g_6', 'monthly_3g_7', 'monthly_3g_8'\n                   , 'sachet_3g_6', 'sachet_3g_7', 'sachet_3g_8', 'aon', 'aug_vbc_3g', 'jul_vbc_3g', 'jun_vbc_3g']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:55:28.409231Z","iopub.execute_input":"2025-02-09T13:55:28.409540Z","iopub.status.idle":"2025-02-09T13:55:28.432388Z","shell.execute_reply.started":"2025-02-09T13:55:28.409516Z","shell.execute_reply":"2025-02-09T13:55:28.431266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X_train.shape)\ndf_cleaned = treat_outliers(X_train[outlier_trt_rqd], method='remove', criteria='boxplot')\ndf_cleaned2 = treat_outliers(X_train[outlier_trt_rqd], method='remove', criteria='zscore')\nprint(df_cleaned.shape, df_cleaned2.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:55:28.433371Z","iopub.execute_input":"2025-02-09T13:55:28.433690Z","iopub.status.idle":"2025-02-09T13:55:29.137732Z","shell.execute_reply.started":"2025-02-09T13:55:28.433665Z","shell.execute_reply":"2025-02-09T13:55:29.136726Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### We will lose a lot of data if we are removing outliers, therefore we will cap the outliers","metadata":{}},{"cell_type":"code","source":"X_train_cleaned = X_train.copy()\nX_train_cleaned[outlier_trt_rqd] = treat_outliers(X_train[outlier_trt_rqd], method='cap', criteria='zscore')\nprint(X_train.shape, X_train_cleaned.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:55:29.138762Z","iopub.execute_input":"2025-02-09T13:55:29.139060Z","iopub.status.idle":"2025-02-09T13:55:29.375669Z","shell.execute_reply.started":"2025-02-09T13:55:29.139037Z","shell.execute_reply":"2025-02-09T13:55:29.374519Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Imputing the rest of the columns with null values by median, this will automatically now take care of categorical variables as well","metadata":{}},{"cell_type":"code","source":"# Dropping three date columns for the time being\nX_train_cleaned = X_train_cleaned.drop(['date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8'], axis = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:55:29.380935Z","iopub.execute_input":"2025-02-09T13:55:29.381254Z","iopub.status.idle":"2025-02-09T13:55:29.407387Z","shell.execute_reply.started":"2025-02-09T13:55:29.381230Z","shell.execute_reply":"2025-02-09T13:55:29.406162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"imputer = SimpleImputer(strategy='median') \nX_train_imputed = imputer.fit_transform(X_train_cleaned)\nX_train_imputed = pd.DataFrame(X_train_imputed)\nX_train_imputed.columns = X_train_cleaned.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:55:29.409102Z","iopub.execute_input":"2025-02-09T13:55:29.409408Z","iopub.status.idle":"2025-02-09T13:55:30.421379Z","shell.execute_reply.started":"2025-02-09T13:55:29.409383Z","shell.execute_reply":"2025-02-09T13:55:30.420467Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Performing PCA to 104 existing columns to reduce the feature list for easy computation while also retaining the variance/ information from the data","metadata":{}},{"cell_type":"markdown","source":"Defining a function in order to see how many KPI's need to be kept","metadata":{}},{"cell_type":"code","source":"def apply_pca(df, variance_threshold=0.95):\n    \"\"\"\n    Applies PCA on a numerical dataframe after scaling and determines the number of components \n    needed to retain the given variance threshold.\n\n    Parameters:\n    df (pd.DataFrame): Input dataframe (should only contain numerical columns).\n    variance_threshold (float): The cumulative variance threshold to retain features (default=95%).\n\n    Returns:\n    pca_df (pd.DataFrame): Transformed dataframe with reduced dimensions.\n    pca_final (PCA object): Fitted PCA model for further transformations.\n    scaler (StandardScaler object): Fitted scaler for future transformations.\n    \"\"\"\n    # Standardizing the features (PCA works better with standardized data)\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df)\n\n    # Apply PCA\n    pca = PCA()\n    pca.fit(scaled_data)\n\n    # Calculate explained variance ratio\n    explained_variance = np.cumsum(pca.explained_variance_ratio_)\n\n    # Find the number of components to keep based on variance threshold\n    num_components = np.argmax(explained_variance >= variance_threshold) + 1\n    print(f\"Optimal number of components to retain {variance_threshold*100}% variance: {num_components}\")\n\n    # Plot explained variance\n    plt.figure(figsize=(8, 5))\n    plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n    plt.axhline(y=variance_threshold, color='r', linestyle='--', label=f'{variance_threshold*100}% Variance')\n    plt.xlabel(\"Number of Principal Components\")\n    plt.ylabel(\"Cumulative Explained Variance\")\n    plt.title(\"Explained Variance vs. Number of Components\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    # Apply PCA with optimal number of components\n    pca_final = PCA(n_components=num_components)\n    pca_data = pca_final.fit_transform(scaled_data)\n\n    # Convert to DataFrame\n    pca_df = pd.DataFrame(pca_data, columns=[f\"PC{i+1}\" for i in range(num_components)])\n    \n    return pca_df, pca_final, scaler ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:55:30.422478Z","iopub.execute_input":"2025-02-09T13:55:30.422885Z","iopub.status.idle":"2025-02-09T13:55:30.431271Z","shell.execute_reply.started":"2025-02-09T13:55:30.422845Z","shell.execute_reply":"2025-02-09T13:55:30.430234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_pca, pca_final, scaler = apply_pca(X_train_imputed, variance_threshold=0.95)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:55:30.432251Z","iopub.execute_input":"2025-02-09T13:55:30.432545Z","iopub.status.idle":"2025-02-09T13:55:32.172173Z","shell.execute_reply.started":"2025-02-09T13:55:30.432521Z","shell.execute_reply":"2025-02-09T13:55:32.171068Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Even after performing PCA, it boils down to around 69 variables for 95% variance, and 37 for 80 percent variance, therefore, process would be to, first do a basic RandomForest Model on 101 features, then we can get feature importances to capture most relevant/ top 80 features and then we can do PCA","metadata":{}},{"cell_type":"markdown","source":"### Base Model -  to check how the results are\n\n#### It is a one time thing, but by getting important variables we will anyway reduce non important features and this is for one time only, no reason for keeping all variables and then having 95% variance if all are not important to us.\n\nThe following snippet is not for testing a model, but to just understand what features come as most important","metadata":{}},{"cell_type":"code","source":"rf_base = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_base.fit(X_train_imputed, y_train)\ny_pred_train = rf_base.predict(X_train_imputed)\n\naccuracy = accuracy_score(y_train, y_pred_train)\nprint(f\"Model Accuracy: {accuracy:.2f}\")\nprint(\"\\nClassification Report:\\n\", classification_report(y_train, y_pred_train))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:55:32.173383Z","iopub.execute_input":"2025-02-09T13:55:32.173871Z","iopub.status.idle":"2025-02-09T13:56:14.269346Z","shell.execute_reply.started":"2025-02-09T13:55:32.173840Z","shell.execute_reply":"2025-02-09T13:56:14.266781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_importance_df = pd.DataFrame({\n    'Feature': X_train_imputed.columns,\n    'Importance': rf_base.feature_importances_\n})\n\nfeature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n\ntop_50_features = feature_importance_df['Feature'].head(50).tolist()\ntop_50_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:56:14.270210Z","iopub.execute_input":"2025-02-09T13:56:14.270564Z","iopub.status.idle":"2025-02-09T13:56:14.299731Z","shell.execute_reply.started":"2025-02-09T13:56:14.270532Z","shell.execute_reply":"2025-02-09T13:56:14.298721Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Creating the final dataset for modeling activity","metadata":{}},{"cell_type":"code","source":"X_train_pre_final = X_train_imputed[top_50_features]\nprint(X_train_pre_final.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:56:14.300855Z","iopub.execute_input":"2025-02-09T13:56:14.301218Z","iopub.status.idle":"2025-02-09T13:56:14.312706Z","shell.execute_reply.started":"2025-02-09T13:56:14.301187Z","shell.execute_reply":"2025-02-09T13:56:14.311561Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_final, pca_final, scaler = apply_pca(X_train_pre_final, variance_threshold=0.90)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:56:14.313936Z","iopub.execute_input":"2025-02-09T13:56:14.314297Z","iopub.status.idle":"2025-02-09T13:56:15.389886Z","shell.execute_reply.started":"2025-02-09T13:56:14.314263Z","shell.execute_reply":"2025-02-09T13:56:15.388788Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Keeping at 90 % variance stored as we noticed overfitting in the base model check to filter important features\n### Selected only top 50 variables from RF model as important because it was clearly overfitting","metadata":{}},{"cell_type":"code","source":"X_train_final.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:56:15.390829Z","iopub.execute_input":"2025-02-09T13:56:15.391129Z","iopub.status.idle":"2025-02-09T13:56:15.418128Z","shell.execute_reply.started":"2025-02-09T13:56:15.391104Z","shell.execute_reply":"2025-02-09T13:56:15.416381Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Modeling -\n#### Now, we will run iterations of modeling activity to check how the models are performing on train and validation set\n#### The best model will be selected for further hyperparameter tuning","metadata":{}},{"cell_type":"markdown","source":"#### Preparing the validation and the test set to be in the same format","metadata":{}},{"cell_type":"code","source":"X_test = X_test[X_train_imputed.columns]\nX_val = X_val[X_train_imputed.columns]\n\nprint(X_val.shape)\nprint(X_test.shape)\nX_val.head(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:56:15.419298Z","iopub.execute_input":"2025-02-09T13:56:15.419692Z","iopub.status.idle":"2025-02-09T13:56:15.499081Z","shell.execute_reply.started":"2025-02-09T13:56:15.419662Z","shell.execute_reply":"2025-02-09T13:56:15.497920Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Impute the variables with the training dataset values","metadata":{}},{"cell_type":"markdown","source":"#### Steps on X_val","metadata":{}},{"cell_type":"code","source":"X_val[['night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', 'fb_user_6', 'fb_user_7', 'fb_user_8']] = X_val[['night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', 'fb_user_6', 'fb_user_7', 'fb_user_8']].fillna(0, axis = 1)\n\n\n\n\nX_val_imputed = imputer.transform(X_val)\nX_val_imputed = pd.DataFrame(X_val_imputed)\nX_val_imputed.columns = X_val.columns\n\nX_val_imputed = X_val[top_50_features]\nX_val_imputed.fillna(0, inplace = True, axis = 1)\nX_val_scaled = scaler.transform(X_val_imputed)\n\nX_val_final = pca_final.transform(X_val_scaled)\nX_val_final = pd.DataFrame(X_val_final, columns=[f\"PC{i+1}\" for i in range(26)])\nprint(X_val_final.shape)\nX_val_final.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:56:15.500300Z","iopub.execute_input":"2025-02-09T13:56:15.500708Z","iopub.status.idle":"2025-02-09T13:56:15.564135Z","shell.execute_reply.started":"2025-02-09T13:56:15.500670Z","shell.execute_reply":"2025-02-09T13:56:15.563033Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Steps on X_test","metadata":{}},{"cell_type":"code","source":"X_test[['night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', 'fb_user_6', 'fb_user_7', 'fb_user_8']] = X_test[['night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', 'fb_user_6', 'fb_user_7', 'fb_user_8']].fillna(0, axis = 1)\n\n\nX_test_imputed = imputer.transform(X_test)\nX_test_imputed = pd.DataFrame(X_test_imputed)\nX_test_imputed.columns = X_test.columns\n\nX_test_imputed = X_test[top_50_features]\nX_test_imputed.fillna(0, inplace = True, axis = 1)\nX_test_scaled = scaler.transform(X_test_imputed)\n\nX_test_final = pca_final.transform(X_test_scaled)\nX_test_final = pd.DataFrame(X_test_final, columns=[f\"PC{i+1}\" for i in range(26)])\nprint(X_test_final.shape)\nX_test_final.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:56:15.565320Z","iopub.execute_input":"2025-02-09T13:56:15.565740Z","iopub.status.idle":"2025-02-09T13:56:15.718482Z","shell.execute_reply.started":"2025-02-09T13:56:15.565699Z","shell.execute_reply":"2025-02-09T13:56:15.717546Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Running through a loop of Classifier models to finalize the best model before going ahead with hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:56:15.719493Z","iopub.execute_input":"2025-02-09T13:56:15.719873Z","iopub.status.idle":"2025-02-09T13:56:15.993688Z","shell.execute_reply.started":"2025-02-09T13:56:15.719823Z","shell.execute_reply":"2025-02-09T13:56:15.992730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_models(models, X_train, y_train, X_test, y_test, cv=5, scoring=\"accuracy\"):\n    \"\"\"\n    Evaluates multiple models using cross-validation on the train set and reports test set scores.\n\n    Parameters:\n    models (dict): Dictionary of models with model names as keys and instances as values.\n    X_train (pd.DataFrame or np.array): Training feature set.\n    y_train (pd.Series or np.array): Training labels.\n    X_test (pd.DataFrame or np.array): Test feature set.\n    y_test (pd.Series or np.array): Test labels.\n    cv (int): Number of cross-validation folds (default=5).\n    scoring (str): Scoring metric for cross-validation (default=\"accuracy\").\n\n    Prints:\n    - Model name\n    - Cross-validation mean score\n    - Test set score\n    \"\"\"\n    for name, model in models.items():\n        # Perform cross-validation on training data\n        cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring)\n        \n        # Train model on full training set\n        model.fit(X_train, y_train)\n        \n        # Evaluate on test set\n        test_score = model.score(X_test, y_test)  # Default method\n        # Alternatively, if using accuracy, use:\n        # test_score = accuracy_score(y_test, model.predict(X_test))\n\n        print(f\"Model: {name}\")\n        print(f\"  - Cross-Validation Score (Mean): {cv_scores.mean():.4f}\")\n        print(f\"  - Test Set Score: {test_score:.4f}\")\n        print(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:56:15.994750Z","iopub.execute_input":"2025-02-09T13:56:15.995041Z","iopub.status.idle":"2025-02-09T13:56:16.001699Z","shell.execute_reply.started":"2025-02-09T13:56:15.995017Z","shell.execute_reply":"2025-02-09T13:56:16.000529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"SVM\": SVC(),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"XGBoost\": XGBClassifier()\n}\n\n# Call the function with training & test data\nevaluate_models(models, X_train_final, y_train, X_val_final, y_val, cv=5, scoring=\"accuracy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:56:16.002797Z","iopub.execute_input":"2025-02-09T13:56:16.003132Z","iopub.status.idle":"2025-02-09T14:15:21.960373Z","shell.execute_reply.started":"2025-02-09T13:56:16.003105Z","shell.execute_reply":"2025-02-09T14:15:21.959519Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Looking at basic results from multiple models, XGBoost seems to be comparatively giving better results. Hence proceeding with XGBoost for further Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"xgb_base = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n\n# Define the hyperparameter grid\nparam_grid = {\n    'n_estimators': [100, 300, 500, 700],  # Number of boosting rounds\n    'max_depth': [3, 5, 7, 9],  # Tree depth\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Step size shrinkage\n    'subsample': [0.6, 0.8, 1.0],  # Fraction of samples used per tree\n    'colsample_bytree': [0.6, 0.8, 1.0],  # Fraction of features used per tree\n    'gamma': [0, 0.1, 0.2, 0.3],  # Minimum loss reduction required to make further partitions\n    'reg_alpha': [0, 0.1, 0.5, 1.0],  # L1 regularization\n    'reg_lambda': [0, 0.1, 0.5, 1.0]  # L2 regularization\n}\n\n# --------------------------------\n# Performing Random Search First to get results faster\n# --------------------------------\nrandom_search = RandomizedSearchCV(\n    estimator=xgb_base,\n    param_distributions=param_grid,\n    n_iter=20,  # Number of random combinations to try\n    scoring='accuracy',\n    cv=5,  # 5-fold Cross-Validation\n    verbose=2,\n    n_jobs=-1,\n    random_state=42\n)\n\n# Fit the model with RandomizedSearchCV\nrandom_search.fit(X_train_final, y_train)\n\n# Print best parameters from Randomized Search\nprint(\"Best Parameters from Randomized Search:\")\nprint(random_search.best_params_)\n\n# --------------------------------\n# Perform Grid Search (Fine-Tuning)\n# --------------------------------\n# Select best parameters from Randomized Search\nbest_params = random_search.best_params_\n\n# Define a refined grid using best params\ngrid_search = GridSearchCV(\n    estimator=XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42),\n    param_grid={\n        'n_estimators': [best_params['n_estimators'] - 100, best_params['n_estimators'], best_params['n_estimators'] + 100],\n        'max_depth': [best_params['max_depth'] - 1, best_params['max_depth'], best_params['max_depth'] + 1],\n        'learning_rate': [best_params['learning_rate'] * 0.8, best_params['learning_rate'], best_params['learning_rate'] * 1.2]\n    },\n    scoring='accuracy',\n    cv=5,\n    verbose=2,\n    n_jobs=-1\n)\n\n# Fit the model with GridSearchCV\ngrid_search.fit(X_train_final, y_train)\n\n# Print best parameters from Grid Search\nprint(\"\\nBest Parameters from Grid Search:\")\nprint(grid_search.best_params_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T14:15:21.961050Z","iopub.execute_input":"2025-02-09T14:15:21.961310Z","iopub.status.idle":"2025-02-09T14:36:56.511577Z","shell.execute_reply.started":"2025-02-09T14:15:21.961288Z","shell.execute_reply":"2025-02-09T14:36:56.510560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_model = XGBClassifier(\n    use_label_encoder=False, \n    eval_metric=\"logloss\",\n    **grid_search.best_params_,\n    # scale_pos_weight = 60,\n    random_state=42\n)\n\nfinal_model.fit(X_train_final, y_train)\n# X_train_final, y_train, X_val_final, y_val\n\n# Predict and Evaluate\ny_pred_train = final_model.predict(X_train_final)\naccuracy_train = accuracy_score(y_train, y_pred_train)\n\nprint(f\"\\nFinal Model Train Accuracy: {accuracy_train:.4f}\")\nprint(confusion_matrix(y_train, y_pred_train))\nprint(classification_report(y_train, y_pred_train))\n\ny_pred_val = final_model.predict(X_val_final)\naccuracy_val = accuracy_score(y_val, y_pred_val)\nprint('*'*60)\n\nprint(f\"\\nFinal Model Val Accuracy: {accuracy_val:.4f}\")\nprint(confusion_matrix(y_val, y_pred_val))\nprint(classification_report(y_val, y_pred_val))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Using predictions on unseen data","metadata":{}},{"cell_type":"code","source":"X_test_final.shape, test_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T14:37:14.942372Z","iopub.execute_input":"2025-02-09T14:37:14.942674Z","iopub.status.idle":"2025-02-09T14:37:14.949499Z","shell.execute_reply.started":"2025-02-09T14:37:14.942649Z","shell.execute_reply":"2025-02-09T14:37:14.948400Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_output.head(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T14:37:14.950923Z","iopub.execute_input":"2025-02-09T14:37:14.951589Z","iopub.status.idle":"2025-02-09T14:37:14.980984Z","shell.execute_reply.started":"2025-02-09T14:37:14.951548Z","shell.execute_reply":"2025-02-09T14:37:14.979838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_test = final_model.predict(X_test_final)\n\ntest_ids = test_df[['id']]\n\noutput = pd.concat([test_ids, pd.DataFrame(y_pred_test)], axis = 1)\noutput.columns = ['id', 'churn_probability']\noutput.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T14:37:14.982339Z","iopub.execute_input":"2025-02-09T14:37:14.982839Z","iopub.status.idle":"2025-02-09T14:37:15.439651Z","shell.execute_reply.started":"2025-02-09T14:37:14.982750Z","shell.execute_reply":"2025-02-09T14:37:15.438787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output.to_csv('priyanujmisra_rahulsrivastava.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T14:37:15.440565Z","iopub.execute_input":"2025-02-09T14:37:15.442903Z","iopub.status.idle":"2025-02-09T14:37:15.487698Z","shell.execute_reply.started":"2025-02-09T14:37:15.442869Z","shell.execute_reply":"2025-02-09T14:37:15.486549Z"}},"outputs":[],"execution_count":null}]}